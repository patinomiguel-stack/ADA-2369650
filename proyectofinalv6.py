# -*- coding: utf-8 -*-
"""ProyectoFinalV6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pbP0IlVvbIUkJiQBJb4wMk2P2L6Ws5lf
"""

import numpy as np
import time
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from collections import Counter
import sys
import copy # Necesario para la verificación de gradientes

# ====================================================================
# CONFIGURACIÓN GLOBAL Y UTILIDADES
# ====================================================================

# Parámetros Globales Fijos para Experimentos de Comparación
H_FIJO = 128
B_FIJO = 64
LR_FIJO = 0.5
NUM_CLASES = 10

# Mapeo de Complejidad para la Consola (Nueva funcionalidad RA5)
# R: mining_ratio, N: Tamaño entrenamiento, B: Tamaño lote, D: Dimensión entrada, H: Oculta, K: Salida, M: Tamaño prueba.
MAPA_COMPLEJIDAD = {
    "MLP PASE_ADELANTE": "O(B*D*H + B*H*K)",
    "MLP ENTRENAMIENTO (EPOCA)": "O(N*D*H + N*H*K)",
    "MLP HARD MINING (EPOCA)": "O(N log K_samples + R*N*(D*H + H*K))",
    "K-NN PREDICCION": "O(M*N*D)",
    "K-NN ENTRENAMIENTO": "O(1)",
    "TOP-K HEAP": "O(N log K_top)",
    "TOP-K SORT": "O(N log N)",
    "QUICKSELECT (MEDIANA)": "O(N)"
}

# --- NUEVO MAPEO PARA EL MENÚ NUMERAL ---
OPCIONES_MENU = {
    str(i+1): alg for i, alg in enumerate(MAPA_COMPLEJIDAD.keys())
}
# ----------------------------------------

def softmax(Z):
    """Función de activación Softmax."""
    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
    A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    return A

def relu(Z):
    """Función de activación ReLU."""
    return np.maximum(0, Z)

def relu_backward(dA, Z):
    """Derivada de ReLU para Backpropagation."""
    dZ = np.array(dA, copy=True)
    dZ[Z <= 0] = 0
    return dZ

def cross_entropy_loss(A, Y):
    """Calcula la pérdida de Entropía Cruzada (escalar)."""
    m = Y.shape[0]
    log_probs = -np.log(A[np.arange(m), np.argmax(Y, axis=1)] + 1e-12)
    costo = np.sum(log_probs) / m
    return costo

def one_hot_encode(y, num_clases):
    """Convierte etiquetas a codificación one-hot."""
    y_int = y.astype(int)
    one_hot = np.zeros((y_int.size, num_clases))
    one_hot[np.arange(y_int.size), y_int] = 1
    return one_hot

# ====================================================================
# FASE 2 & 3: ESTRUCTURAS DE DATOS (MinHeap y Quickselect)
# ====================================================================

class MinHeap:
    def __init__(self, k):
        self.monticulo = []
        self.k = k

    def _heapify(self, i):
        n = len(self.monticulo)
        menor = i
        izquierda = 2 * i + 1
        derecha = 2 * i + 2
        if izquierda < n and self.monticulo[izquierda][0] < self.monticulo[menor][0]:
            menor = izquierda
        if derecha < n and self.monticulo[derecha][0] < self.monticulo[menor][0]:
            menor = derecha
        if menor != i:
            self.monticulo[i], self.monticulo[menor] = self.monticulo[menor], self.monticulo[i]
            self._heapify(menor)

    def insert(self, valor, indice):
        item = (valor, indice)
        if len(self.monticulo) < self.k:
            self.monticulo.append(item)
            i = len(self.monticulo) - 1
            while i > 0 and self.monticulo[(i - 1) // 2][0] > self.monticulo[i][0]:
                self.monticulo[i], self.monticulo[(i - 1) // 2] = self.monticulo[(i - 1) // 2], self.monticulo[i]
                i = (i - 1) // 2
        elif valor > self.monticulo[0][0]:
            self.monticulo[0] = item
            self._heapify(0)

    def get_indices(self):
        return [item[1] for item in self.monticulo]

    def get_valores(self):
        return [item[0] for item in self.monticulo]

class QuickSelect:
    """
    Implementación del algoritmo Quickselect para encontrar el k-ésimo elemento
    (o la mediana) en tiempo O(N) promedio. (RA2)
    """
    def __init__(self, arr):
        self.arr = np.copy(arr)

    def _particion(self, bajo, alto):
        """Partición Lomuto, similar a Quicksort."""
        pivote = self.arr[alto]
        i = bajo - 1
        for j in range(bajo, alto):
            if self.arr[j] <= pivote:
                i += 1
                self.arr[i], self.arr[j] = self.arr[j], self.arr[i]
        self.arr[i + 1], self.arr[alto] = self.arr[alto], self.arr[i + 1]
        return i + 1

    def encontrar_k_esimo(self, k):
        """Encuentra el k-ésimo elemento (0-indexado)"""
        k_indice = k - 1
        bajo, alto = 0, len(self.arr) - 1
        if k_indice < 0 or k_indice >= len(self.arr):
            raise IndexError("k está fuera de rango")

        while bajo <= alto:
            pivote_indice = self._particion(bajo, alto)
            if pivote_indice == k_indice:
                return self.arr[pivote_indice]
            elif pivote_indice < k_indice:
                bajo = pivote_indice + 1
            else:
                alto = pivote_indice - 1
        return None

    def encontrar_mediana(self):
        """Función auxiliar para encontrar la mediana (k = N/2)"""
        n = len(self.arr)
        k = (n + 1) // 2
        return self.encontrar_k_esimo(k)

# ====================================================================
# FASE 1: CLASE MLP BASE (SimpleMLP)
# ====================================================================

class SimpleMLP:
    def __init__(self, tamano_entrada, tamano_oculta, tamano_salida, tasa_aprendizaje):
        self.D = tamano_entrada
        self.H = tamano_oculta
        self.K = tamano_salida
        self.lr = tasa_aprendizaje
        self.W1 = np.random.randn(self.D, self.H) * np.sqrt(2.0/self.D)
        self.b1 = np.zeros((1, self.H))
        self.W2 = np.random.randn(self.H, self.K) * np.sqrt(2.0/self.H)
        self.b2 = np.zeros((1, self.K))

    def _pase_adelante(self, X):
        Z1 = X @ self.W1 + self.b1
        A1 = relu(Z1)
        Z2 = A1 @ self.W2 + self.b2
        A2 = softmax(Z2)
        cache = (X, Z1, A1, Z2, A2)
        return A2, cache

    def _pase_atras(self, Y, A2, cache):
        X, Z1, A1, _, _ = cache
        m = X.shape[0]
        dZ2 = A2 - Y
        dW2 = (A1.T @ dZ2) / m
        db2 = np.sum(dZ2, axis=0, keepdims=True) / m
        dA1 = dZ2 @ self.W2.T
        dZ1 = relu_backward(dA1, Z1)
        dW1 = (X.T @ dZ1) / m
        db1 = np.sum(dZ1, axis=0, keepdims=True) / m
        gradientes = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
        return gradientes

    def _actualizar_parametros(self, gradientes):
        self.W1 -= self.lr * gradientes["dW1"]
        self.b1 -= self.lr * gradientes["db1"]
        self.W2 -= self.lr * gradientes["dW2"]
        self.b2 -= self.lr * gradientes["db2"]

    def train(self, X_entrenamiento, Y_entrenamiento, epocas, tamano_lote, verboso=True):
        m = X_entrenamiento.shape[0]
        for e in range(1, epocas + 1):
            permutacion = np.random.permutation(m)
            X_mezclado = X_entrenamiento[permutacion]
            Y_mezclado = Y_entrenamiento[permutacion]
            perdida_epoca = 0
            for i in range(0, m, tamano_lote):
                X_lote = X_mezclado[i:i + tamano_lote]
                Y_lote = Y_mezclado[i:i + tamano_lote]
                A2, cache = self._pase_adelante(X_lote)
                perdida = cross_entropy_loss(A2, Y_lote)
                perdida_epoca += perdida * X_lote.shape[0]
                gradientes = self._pase_atras(Y_lote, A2, cache)
                self._actualizar_parametros(gradientes)
            if verboso and e % 10 == 0:
                print(f"  > Época {e}/{epocas}, Pérdida: {perdida_epoca / m:.4f}")
        return perdida_epoca / m

    # ====================================================================
    # CORRECCIÓN DE ERROR Y REQUISITO DE CORRECTITUD (Verificación de Gradientes)
    # ====================================================================
    def gradient_check(self, X, Y, epsilon=1e-7):
        """
        Realiza la verificación numérica de los gradientes (gradiente checking).
        Compara el gradiente analítico (backprop) con el gradiente numérico.
        """
        # 1. Obtener gradientes analíticos (Backpropagation)
        A2, cache = self._pase_adelante(X)
        gradientes_analiticos = self._pase_atras(Y, A2, cache)

        # Usamos referencias a los parámetros de la clase
        parametros = [("W1", self.W1), ("b1", self.b1), ("W2", self.W2), ("b2", self.b2)]

        print("\n--- Verificación de Gradientes (Check Numérico) ---")
        print(f"Usando {X.shape[0]} muestras y epsilon={epsilon}")

        passed = True

        for nombre, matriz_parametro in parametros:
            gradiente_analitico = gradientes_analiticos["d" + nombre]
            gradiente_numerico = np.zeros_like(matriz_parametro)

            # Recorrer cada elemento de la matriz de parámetros
            it = np.nditer(matriz_parametro, flags=['multi_index'], op_flags=['readwrite'])

            while not it.finished:
                idx = it.multi_index

                # ----------------------------------------------------
                # CÁLCULO NUMÉRICO: (J(theta + epsilon) - J(theta - epsilon)) / 2*epsilon
                # ----------------------------------------------------
                valor_original = matriz_parametro[idx]

                # Perturbar: theta_plus
                matriz_parametro[idx] = valor_original + epsilon
                A2_plus, _ = self._pase_adelante(X)
                costo_plus = cross_entropy_loss(A2_plus, Y)

                # Perturbar: theta_minus
                matriz_parametro[idx] = valor_original - epsilon
                A2_minus, _ = self._pase_adelante(X)
                costo_minus = cross_entropy_loss(A2_minus, Y)

                # Calcular gradiente numérico
                grad_num = (costo_plus - costo_minus) / (2 * epsilon)
                gradiente_numerico[idx] = grad_num

                # Restaurar valor original (¡CRÍTICO!)
                matriz_parametro[idx] = valor_original

                # Avanzar al siguiente elemento (CORREGIDO: it.iternext() es el método correcto)
                it.iternext()


            # Cálculo de la diferencia relativa
            norm_num = np.linalg.norm(gradiente_numerico)
            norm_analitico = np.linalg.norm(gradiente_analitico)

            # Diferencia relativa (se añade 1e-12 para evitar división por cero)
            diferencia_relativa = np.linalg.norm(gradiente_analitico - gradiente_numerico) / (norm_analitico + norm_num + 1e-12)

            # Evaluación: si la diferencia es mayor a 1e-7, es una advertencia.
            if diferencia_relativa > 1e-7:
                print(f"¡ADVERTENCIA! Falló la verificación de gradientes para {nombre}.")
                print(f" - Diferencia Relativa: {diferencia_relativa:.10f} (Debería ser < 1e-7)")
                passed = False
            else:
                print(f"✅ Gradientes para {nombre} OK. Diferencia Relativa: {diferencia_relativa:.10f}")

        if passed:
            print("\nResultado: ¡La verificación de gradientes fue exitosa para todos los parámetros!")
        else:
            print("\nResultado: ¡La verificación de gradientes falló para al menos un parámetro!")
        print("-" * 50)
        return passed

    def predict(self, X):
        A2, _ = self._pase_adelante(X)
        return np.argmax(A2, axis=1)

# ====================================================================
# FASE 3: MLP CON HARD MINING
# ====================================================================

class MLPWithHardMining(SimpleMLP):
    def __init__(self, *args, razon_minado=0.1, **kwargs):
        super().__init__(*args, **kwargs)
        self.razon_minado = razon_minado

    def _calcular_perdidas_muestra(self, A, Y):
        m = Y.shape[0]
        return -np.log(A[np.arange(m), np.argmax(Y, axis=1)] + 1e-12)

    def train_hard_mining(self, X_entrenamiento, Y_entrenamiento, epocas, tamano_lote):
        m = X_entrenamiento.shape[0]
        for e in range(1, epocas + 1):
            k_muestras = int(m * self.razon_minado)
            monticulo_hard_mining = MinHeap(k_muestras)
            for i in range(0, m, tamano_lote):
                X_lote = X_entrenamiento[i:i + tamano_lote]
                Y_lote = Y_entrenamiento[i:i + tamano_lote]
                A2, _ = self._pase_adelante(X_lote)
                perdidas_muestra = self._calcular_perdidas_muestra(A2, Y_lote)
                indices_globales = np.arange(i, min(i + X_lote.shape[0], m))
                for perdida, indice in zip(perdidas_muestra, indices_globales):
                    monticulo_hard_mining.insert(perdida, indice)

            indices_dificiles = monticulo_hard_mining.get_indices()
            X_dificil = X_entrenamiento[indices_dificiles]
            Y_dificil = Y_entrenamiento[indices_dificiles]
            m_dificil = X_dificil.shape[0]

            perdida_epoca = 0
            for i in range(0, m_dificil, tamano_lote):
                X_lote = X_dificil[i:i + tamano_lote]
                Y_lote = Y_dificil[i:i + tamano_lote]
                A2, cache = self._pase_adelante(X_lote)
                perdida = cross_entropy_loss(A2, Y_lote)
                perdida_epoca += perdida * X_lote.shape[0]
                gradientes = self._pase_atras(Y_lote, A2, cache)
                self._actualizar_parametros(gradientes)

            perdida_promedio = perdida_epoca / m_dificil
            if e % 10 == 0:
                print(f"  > Época {e}/{epocas}, Pérdida HARD: {perdida_promedio:.4f}, Muestras entrenadas: {m_dificil}")
        return perdida_promedio

# ====================================================================
# FASE 4: BASELINE (K-NEAREST NEIGHBORS)
# ====================================================================

class KNearestNeighbors:
    def __init__(self, k=5):
        self.k = k

    def fit(self, X, Y_one_hot):
        self.X_entrenamiento = X
        self.y_entrenamiento = np.argmax(Y_one_hot, axis=1)

    def _calcular_distancia(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2)**2))

    def predict(self, X_prueba):
        predicciones = [self._predecir_uno(x) for x in X_prueba]
        return np.array(predicciones)

    def _predecir_uno(self, x_prueba):
        distancias = []
        for i, x_entrenamiento in enumerate(self.X_entrenamiento):
            dist = self._calcular_distancia(x_prueba, x_entrenamiento)
            distancias.append((dist, self.y_entrenamiento[i]))
        distancias.sort(key=lambda x: x[0])
        k_mas_cercanos = distancias[:self.k]
        clases_vecinas = [etiqueta for dist, etiqueta in k_mas_cercanos]
        mas_comun = Counter(clases_vecinas).most_common(1)
        return mas_comun[0][0]


# ====================================================================
# NUEVA FUNCIONALIDAD: ANÁLISIS INTERACTIVO DE COMPLEJIDAD
# ====================================================================

def analizar_complejidad_desde_consola():
    """
    Función interactiva para que el usuario consulte la complejidad de los algoritmos
    implementados en el proyecto (RA1-RA4) utilizando un menú numeral.
    """
    print("\n" + "="*50)
    print("ANÁLISIS INTERACTIVO DE COMPLEJIDAD (RA5)")
    print("="*50)
    print("Algoritmos disponibles para consulta:")
    print("-" * 50)

    # Mostrar el menú numeral
    for num, alg in OPCIONES_MENU.items():
        print(f"  [{num}] - {alg}")

    print("-" * 50)
    print("Ingrese el número de opción para ver su complejidad (o '0' para salir):")

    while True:
        try:
            entrada_usuario = input("> ").strip()

            if entrada_usuario == '0':
                print("Análisis de complejidad finalizado.")
                break

            # Buscar el nombre del algoritmo por el número de opción
            algoritmo_seleccionado = OPCIONES_MENU.get(entrada_usuario)

            if algoritmo_seleccionado:
                print("\n[RESULTADO]")
                print(f"  Algoritmo: {algoritmo_seleccionado}")
                print(f"  Complejidad Temporal: {MAPA_COMPLEJIDAD[algoritmo_seleccionado]}")
                print("\n(N: Train Size, B: Batch Size, D: Input Dim, H: Hidden Dim, M: Test Size, R: Mining Ratio)")
                print("-" * 50)
                print("Ingrese el número de otra opción (o '0' para salir):")
            else:
                print(f"Error: Opción '{entrada_usuario}' no válida. Ingrese un número de 1 a {len(OPCIONES_MENU)} o '0' para salir.")
        except EOFError:
            print("\nAnálisis de complejidad finalizado (EOF).")
            break
        except Exception as e:
            print(f"Ocurrió un error inesperado: {e}")
            break


# ====================================================================
# EJECUCIÓN DE TODOS LOS EXPERIMENTOS (Resto de las fases)
# ====================================================================

def cargar_y_preprocesar_datos():
    print("1. Cargando y preprocesando Fashion-MNIST...")
    X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, parser='auto')
    X = X.to_numpy(dtype=np.float64) / 255.0
    Y_one_hot = one_hot_encode(y, NUM_CLASES)
    # Reducimos el set de entrenamiento para acelerar los experimentos
    X_entrenamiento_completo, X_prueba_completo, Y_entrenamiento_completo, Y_prueba_completo = train_test_split(
        X, Y_one_hot, test_size=0.8, random_state=42)
    # Usamos un set de prueba muy pequeño para k-NN
    X_prueba_k_nn, _, Y_prueba_k_nn, _ = train_test_split(
        X_prueba_completo, Y_prueba_completo, test_size=0.9, random_state=42)
    TAMANO_ENTRADA = X_entrenamiento_completo.shape[1]
    print(f"    > Dataset de Entrenamiento (N): {X_entrenamiento_completo.shape[0]} muestras")
    print(f"    > Dataset de Prueba (M): {X_prueba_k_nn.shape[0]} muestras para k-NN")
    print("-" * 50)
    return X_entrenamiento_completo, Y_entrenamiento_completo, X_prueba_k_nn, Y_prueba_k_nn, TAMANO_ENTRADA

def ejecutar_experimentos_ra1(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA):
    print("2. FASE 1 (RA1): Experimentos de Escalado (O(B) y O(H))...")
    H_valores = [32, 64, 128, 256, 512]
    tiempos_H = []
    for H in H_valores:
        mlp = SimpleMLP(TAMANO_ENTRADA, H, NUM_CLASES, LR_FIJO)
        tiempo_inicio = time.time()
        mlp.train(X_entrenamiento, Y_entrenamiento, epocas=10, tamano_lote=B_FIJO, verboso=False)
        tiempos_H.append(time.time() - tiempo_inicio)
        print(f"  > H={H}: Tiempo = {tiempos_H[-1]:.4f}s")

    B_valores = [16, 32, 64, 128, 256]
    tiempos_B = []
    for B in B_valores:
        mlp = SimpleMLP(TAMANO_ENTRADA, H_FIJO, NUM_CLASES, LR_FIJO)
        tiempo_inicio = time.time()
        mlp.train(X_entrenamiento, Y_entrenamiento, epocas=10, tamano_lote=B, verboso=False)
        tiempos_B.append(time.time() - tiempo_inicio)
        print(f"  > B={B}: Tiempo = {tiempos_B[-1]:.4f}s")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle('Fase 1: Validación Empírica de Complejidad (O(B*D*H))', fontsize=14)
    ax1.plot(H_valores, tiempos_H, marker='o'); ax1.set_title('Tiempo vs. H (O(H))'); ax1.set_xlabel('H'); ax1.set_ylabel('Tiempo (s)'); ax1.grid(True)
    ax2.plot(B_valores, tiempos_B, marker='o'); ax2.set_title('Tiempo vs. B (O(B))'); ax2.set_xlabel('B'); ax2.set_ylabel('Tiempo (s)'); ax2.grid(True)
    # plt.show() # Descomentar para ver gráficas

    print("-" * 50)

def ejecutar_experimento_quickselect():
    print("3. FASE 2 (RA2): Experimento Quickselect (O(N) promedio)...")
    N_valores = [10000, 50000, 100000, 500000, 1000000]
    tiempos_qs = []
    tiempos_sort = []
    for N in N_valores:
        arr = np.random.rand(N)

        tiempo_inicio_qs = time.time()
        qs = QuickSelect(arr)
        qs.encontrar_mediana()
        tiempos_qs.append(time.time() - tiempo_inicio_qs)

        tiempo_inicio_sort = time.time()
        np.median(arr)
        tiempos_sort.append(time.time() - tiempo_inicio_sort)

        print(f"  > N={N}: Quickselect = {tiempos_qs[-1]:.6f}s")

    plt.figure(figsize=(7, 5))
    plt.plot(N_valores, tiempos_qs, marker='o', label='Quickselect (O(N))')
    plt.plot(N_valores, tiempos_sort, marker='s', label='NumPy Sort/Median (O(N log N))')
    plt.title('Fase 2: Comparación Mediana (Quickselect vs Sort)'); plt.xlabel('N (Tamaño de la lista)'); plt.ylabel('Tiempo (s)'); plt.legend(); plt.grid(True)
    # plt.show() # Descomentar para ver gráficas
    print("-" * 50)


def ejecutar_experimentos_ra2():
    print("4. FASE 2 (RA2): Experimento Top-k (Heap vs. Sort)...")
    N_valores = [1000, 5000, 10000, 50000, 100000]
    K_fijo = 100
    tiempos_monticulo = []
    tiempos_ordenamiento = []
    for N in N_valores:
        arr = np.random.rand(N)
        tiempo_inicio = time.time()
        monticulo = MinHeap(K_fijo)
        for val in arr:
            monticulo.insert(val, 0)
        tiempos_monticulo.append(time.time() - tiempo_inicio)
        tiempo_inicio = time.time()
        np.sort(arr)[::-1][:K_fijo]
        tiempos_ordenamiento.append(time.time() - tiempo_inicio)
        print(f"  > N={N}: Heap = {tiempos_monticulo[-1]:.6f}s, Sort = {tiempos_ordenamiento[-1]:.6f}s")
    plt.figure(figsize=(7, 5))
    plt.plot(N_valores, tiempos_monticulo, marker='o', label='Heap (O(n log k))')
    plt.plot(N_valores, tiempos_ordenamiento, marker='s', label='Sort (O(n log n))')
    plt.title('Fase 2: Comparación Top-k (Heap vs Sort)'); plt.xlabel('N (Tamaño de la lista)'); plt.ylabel('Tiempo (s)'); plt.legend(); plt.grid(True)
    # plt.show() # Descomentar para ver gráficas
    print("-" * 50)

def ejecutar_experimento_ra3(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA):
    print("5. FASE 3 (RA3): Experimento Hard Mining (Integración Heap)...")
    EPOCAS = 10 # Reducido para acelerar el ejemplo
    RAZON_MINADO = 0.1
    print("  > Baseline (SGD Clásico)...")
    mlp_base = SimpleMLP(TAMANO_ENTRADA, H_FIJO, NUM_CLASES, LR_FIJO)
    tiempo_inicio_base = time.time()
    mlp_base.train(X_entrenamiento, Y_entrenamiento, epocas=EPOCAS, tamano_lote=B_FIJO, verboso=False)
    tiempo_base = time.time() - tiempo_inicio_base
    print("  > Hard Mining (RA3 Heap)...")
    mlp_hard = MLPWithHardMining(TAMANO_ENTRADA, H_FIJO, NUM_CLASES, LR_FIJO, razon_minado=RAZON_MINADO)
    tiempo_inicio_hard = time.time()
    mlp_hard.train_hard_mining(X_entrenamiento, Y_entrenamiento, epocas=EPOCAS, tamano_lote=B_FIJO)
    tiempo_hard = time.time() - tiempo_inicio_hard
    print("\n  --- ANÁLISIS DE IMPACTO RA3 ---")
    print(f"  Tiempo Total (Clásico): {tiempo_base:.4f}s")
    print(f"  Tiempo Total (Hard Mining, 10%): {tiempo_hard:.4f}s")
    print(f"  Ahorro de Tiempo: {((tiempo_base - tiempo_hard) / tiempo_base) * 100:.2f}%")
    print("-" * 50)

def ejecutar_comparacion_ra4(X_entrenamiento, Y_entrenamiento, X_prueba, Y_prueba, TAMANO_ENTRADA):
    print("6. FASE 4 (RA4): Comparación Final (MLP vs k-NN Baseline)...")
    N_comp = 5000
    M_comp = 500
    X_entrenamiento_comp, Y_entrenamiento_comp = X_entrenamiento[:N_comp], Y_entrenamiento[:N_comp]
    X_prueba_comp, Y_prueba_comp = X_prueba[:M_comp], Y_prueba[:M_comp]
    Y_prueba_indices_reales = np.argmax(Y_prueba_comp, axis=1)
    EPOCAS_MLP = 10 # Reducido para acelerar el ejemplo

    # MLP Training
    mlp = SimpleMLP(TAMANO_ENTRADA, H_FIJO, NUM_CLASES, LR_FIJO)
    tiempo_inicio_entrenamiento_mlp = time.time()
    mlp.train(X_entrenamiento_comp, Y_entrenamiento_comp, epocas=EPOCAS_MLP, tamano_lote=B_FIJO, verboso=False)
    tiempo_entrenamiento_mlp = time.time() - tiempo_inicio_entrenamiento_mlp
    tiempo_inicio_prediccion_mlp = time.time()
    Y_pred_mlp = mlp.predict(X_prueba_comp)
    tiempo_prediccion_mlp = time.time() - tiempo_inicio_prediccion_mlp
    precision_mlp = np.mean(Y_pred_mlp == Y_prueba_indices_reales)

    # k-NN Baseline
    K_NN = 5
    knn = KNearestNeighbors(k=K_NN)
    tiempo_inicio_entrenamiento_knn = time.time()
    knn.fit(X_entrenamiento_comp, Y_entrenamiento_comp)
    tiempo_entrenamiento_knn = time.time() - tiempo_inicio_entrenamiento_knn # O(1)
    tiempo_inicio_prediccion_knn = time.time()
    Y_pred_knn = knn.predict(X_prueba_comp)
    tiempo_prediccion_knn = time.time() - tiempo_inicio_prediccion_knn
    precision_knn = np.mean(Y_pred_knn == Y_prueba_indices_reales)

    print("\n  --- TABLA DE COMPARACIÓN ALGÍTMICA (MLP vs k-NN) ---")
    print("-" * 65)
    print(f"| {'Métrica':<25} | {'MLP (RA1)':<15} | {'k-NN (RA4 Baseline)':<18} |")
    print("-" * 65)
    print(f"| {'Tiempo de Entrenamiento (s)':<25} | {tiempo_entrenamiento_mlp:<15.4f} | {tiempo_entrenamiento_knn:<18.4f} |")
    print(f"| {'Tiempo de Predicción (s)':<25} | {tiempo_prediccion_mlp:<15.4f} | {tiempo_prediccion_knn:<18.4f} |")
    print(f"| {'Precisión (Accuracy)':<25} | {precision_mlp*100:<15.2f}% | {precision_knn*100:<18.2f}% |")
    print("-" * 65)
    metricas = ['Tiempo Predicción (s)', 'Precisión (%)']
    valores_mlp = [tiempo_prediccion_mlp, precision_mlp * 100]
    valores_knn = [tiempo_prediccion_knn, precision_knn * 100]
    fig, ax = plt.subplots(figsize=(8, 5))
    x = np.arange(len(metricas)); width = 0.35
    ax.bar(x - width/2, valores_mlp, width, label='MLP'); ax.bar(x + width/2, valores_knn, width, label='k-NN')
    ax.set_title('Fase 4: Comparación Final MLP vs k-NN'); ax.set_xticks(x); ax.set_xticklabels(metricas); ax.legend()
    # plt.show() # Descomentar para ver gráficas

def ejecutar_verificacion_gradientes(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA):
    print("7. VERIFICACIÓN DE GRADIENTES (Correctitud)...")
    # Usar un dataset muy pequeño para que la verificación numérica no tarde demasiado
    N_check = 10
    X_check = X_entrenamiento[:N_check]
    Y_check = Y_entrenamiento[:N_check]

    # MLP simple con pocas unidades ocultas para acelerar el chequeo (D=784, H=10, K=10)
    mlp = SimpleMLP(TAMANO_ENTRADA, tamano_oculta=10, tamano_salida=NUM_CLASES, tasa_aprendizaje=0.1)

    mlp.gradient_check(X_check, Y_check)
    print("-" * 50)


if __name__ == "__main__":
    X_entrenamiento, Y_entrenamiento, X_prueba, Y_prueba, TAMANO_ENTRADA = cargar_y_preprocesar_datos()

    # 1. Ejecutar las fases de análisis (RA1-RA4)
    ejecutar_experimentos_ra1(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA)
    ejecutar_experimento_quickselect()
    ejecutar_experimentos_ra2()
    ejecutar_experimento_ra3(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA)
    ejecutar_comparacion_ra4(X_entrenamiento, Y_entrenamiento, X_prueba, Y_prueba, TAMANO_ENTRADA)

    # 2. Requisito de Correctitud (RA1)
    ejecutar_verificacion_gradientes(X_entrenamiento, Y_entrenamiento, TAMANO_ENTRADA)

    # 3. Ejecutar el análisis interactivo (RA5)
    analizar_complejidad_desde_consola()